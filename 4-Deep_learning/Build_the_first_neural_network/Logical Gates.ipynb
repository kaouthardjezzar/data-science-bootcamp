{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your first Neural Network\n",
    "In this notebook, you will build your first neural network that will perform the function of a logical gate.\n",
    "You will implement this neural network from scratch and this will let you have a better understending of the different concepts you have seen in this chapter such as parameters' initialization, forward, and backward progpagation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Gates (Reminder)\n",
    "Logic gates are the basic building blocks of any digital system. It is an electronic circuit having one or more than one input and only one output. The table below summerizes the existing logical gates and their truth tables:\n",
    "![Logic Gates!](./images/logic_gates.jpeg \"Logic Gates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network Architecture\n",
    "We are going to implement an AND gate, you can look at its truth table above. Our neural network will have 1 hidden layer which has 2 neurons, and its input will have two features.\n",
    "![AND Gate NN architecture!](./images/AND_Gate_NN_architecture.png \"AND Gate NN architecture\")\n",
    "The goal is to automatically adjust those weights and biases by executing forward and backward passes on data points that reprensent the AND gate truth's table so that the output of this NN will match the AND gate output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the sigmoid activation function. Here is its formula:\n",
    "$S(x)= \\frac {1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that initializes the parameters of our neural network.\n",
    "- All the weights will be initialized randomly.\n",
    "- Bias values will be initialized to 0.\n",
    "- We can use one of the initilization we talked about previously, like Xavier initialization, but for simplicity let's start that way.\n",
    "\n",
    "To keep the function we are defining usable if we have more or less input or output features, or if we decide to add more neurons in the hidden layer, we pass three parameters to the function:\n",
    "\n",
    "- inputSize: specifies how many features does the input have. (2 in our case)\n",
    "- hiddenLayerSize: specifies how many neurons the hidden layer have. (2 in our case)\n",
    "- outputSize: specifies how many features does the output have. (1 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParameters(inputSize, hiddenLayerSize, outputSize):\n",
    "    W1 = np.random.randn(hiddenLayerSize, inputSize)\n",
    "    W2 = np.random.randn(outputSize, hiddenLayerSize)\n",
    "    b1 = np.zeros((hiddenLayerSize, 1))\n",
    "    b2 = np.zeros((outputSize, 1))\n",
    "    # We store our parameters in a dict and return them\n",
    "    parameters = {\"W1\" : W1, \"b1\": b1, \"W2\" : W2, \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the forward pass function.\n",
    "- We calculate $A_{2}$ as detailed in the architecture figure\n",
    "- We save all the calculations' results so won't have to calculate them again during the back propagation in the cache variable\n",
    "- We calculate the cross entropy for our data points: $-(y \\times log(\\hat{y}) + (1-y) \\times (log(1-\\hat{y}))$\n",
    "- The cost is defined as the mean of these cross entropies, so we devide their sum by the number of our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(X, Y, parameters):\n",
    "    W1, W2, b1, b2 = parameters[\"W1\"], parameters[\"W2\"], parameters[\"b1\"], parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
    "    cross_entropies = - (np.multiply(Y, np.log(A2)) + np.multiply((1 - Y), np.log(1 - A2)))\n",
    "    cost = np.sum(cross_entropies) / X.shape[1]\n",
    "    return cost, cache, A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we implement the backward pass function.\n",
    "- The cached results from the forward pass are passed to the backward as parameters\n",
    "- The formulas of the gradients are detailed in the previous lesson, we just need to implement them.\n",
    "- We set the $keepdims$ parameter to True when summing to ensure the resulted array will have the same dimensions as the input array. For example summing a (2,3) matrix over axis 1 while give us a (2,1) matrix when $keepdims$ is set to true instead of a 1D array with 2 elements.\n",
    "- Finally, we return the gradients as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPass(X, Y, cache):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, A1 * (1- A1))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the parametes using the gradient descent algorithm after each iteration, let's implement a function that updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, gradients, learningRate):\n",
    "    parameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"]\n",
    "    parameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"]\n",
    "    parameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
    "    parameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented all the prerequesties to train and test our neural network. Let's define the data point that we will train it on!\n",
    "As we want our neural network to perform the AND operation, we train it the four possible inputs of this logic gate.\n",
    "- $0 \\wedge  0 = 0$\n",
    "- $0 \\wedge  1 = 0$\n",
    "- $1 \\wedge  0 = 0$\n",
    "- $1 \\wedge  1 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) \n",
    "Y = np.array([[0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initilize the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayerSize = 2 # number of hidden layer neurons (2)\n",
    "inputSize = X.shape[0] # number of input features (2)\n",
    "outputSize = Y.shape[0] # number of output features (1)\n",
    "parameters = initializeParameters(inputSize, hiddenLayerSize, outputSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we start the training:\n",
    "- We will run 100000 iterations with a learning rate of 0.01. \n",
    "- We keep track of the losses so we can plot them later.\n",
    "- The training consists of running a forward and a backward pass on all the data points and update the parameters at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100000\n",
    "learningRate = 0.01\n",
    "losses = np.zeros((epoch, 1))\n",
    "\n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardPass(X, Y, parameters)\n",
    "    gradients = backwardPass(X, Y, cache)\n",
    "    parameters = updateParameters(parameters, gradients, learningRate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of our model by visualizing the change in the loss value after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our model by running a forward pass on data points of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0052867465510390505\n",
      "[[0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 1, 1], [0, 0, 1]])\n",
    "Y = np.array([[0,0,1]])\n",
    "cost, _, A2 = forwardPass(X, Y, parameters)\n",
    "prediction = (A2 > 0.5) * 1.0\n",
    "print(cost)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
